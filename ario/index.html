<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ARIO</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">All Robots in One: A New Standard and Unified Dataset for
              Versatile, General-Purpose Embodied Agents</h1>
            <!-- <img src="static/images/logov1.png" style="width: 10%; height: auto;"> 
            <br> -->
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">Hua Ye<sup>1*</sup>,</span>
              <span class="author-block">Zhiqiang Wang<sup>2*</sup>,</span>
              <span class="author-block">Hao Zheng<sup>2*</sup>,</span>
              <span class="author-block">Yunshuang Nie<sup>3*</sup>,</span>
              <span class="author-block">Wenjun Xu<sup>1*</sup></span>
              <span class="author-block">Qingwei Wang<sup>2*</sup>,</span>
              <span class="author-block">Liang Lin<sup>1,3</sup>,</span>
              <span class="author-block">Feng Zheng<sup>1,2</sup><sup>†</sup>,</span>
              <span class="author-block">Xiaodan Liang<sup>1,3</sup><sup>†</sup>,</span>
              <span class="author-block">Zhe Li<sup>2</sup>,</span>
              <span class="author-block">Kaidong Zhang<sup>3</sup>,</span>
              <span class="author-block">Jinrui Zhang<sup>2</sup>,</span>
              <span class="author-block">Xuewen Cheng<sup>1</sup>,</span>
              <span class="author-block">Wanxi Dong<sup>2</sup>,</span>
              <span class="author-block">Chang Cai<sup>1</sup>,</span>
              <span class="author-block">Tingting Shen<sup>2</sup>,</span>
              <span class="author-block">Zhen Luo<sup>2</sup>,</span>
              <span class="author-block">Fangjing Wang<sup>2</sup>,</span>
              <span class="author-block">Zesheng Yang<sup>2</sup>,</span>
              <span class="author-block">Liang Xu<sup>2</sup>,</span>
              <span class="author-block">Yi Yan<sup>2</sup>,</span>
              <span class="author-block">Yixuan Yang<sup>2</sup>,</span>
              <span class="author-block">Jingnan Luo<sup>2</sup>,</span>
              <span class="author-block">Tongsheng Ding<sup>2</sup>,</span>
              <span class="author-block">Ziwei Chen<sup>2</sup>,</span>
              <span class="author-block">Guoyu Xiong<sup>2</sup>,</span>
              <span class="author-block">Xi Jiang<sup>2</sup>,</span>
              <span class="author-block">Tiantian Geng<sup>2</sup>,</span>
              <span class="author-block">Zhenhong Guo<sup>2</sup>,</span>
              <span class="author-block">Xue Jiang<sup>2</sup>,</span>
              <span class="author-block">Zhengyu Lin<sup>2</sup>,</span>
              <span class="author-block">Ziling Liu<sup>2</sup>,</span>
              <span class="author-block">Junfan Lin<sup>1</sup>,</span>
              <span class="author-block">Qingyong Jia<sup>1</sup>,</span>
              <span class="author-block">Yazhan Zhang<sup>1</sup>,</span>
              <span class="author-block">Jichang Li<sup>1</sup>,</span>
              <span class="author-block">Bingyi Xia<sup>2</sup>,</span>
              <span class="author-block">Jingyi Liu<sup>2</sup>,</span>
              <span class="author-block">Shiwei Zhang<sup>3</sup>,</span>
              <span class="author-block">Yun Pei<sup>3</sup>,</span>
              <span class="author-block">Yao Xiao<sup>3</sup>,</span>
            </div>
            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Research Institute of Multiple Agents and Embodied Intelligence,
                Peng Cheng Laboratory,<br>(鹏城实验室多智能体与具身智能研究所)</span>
              <br>
              <span class="author-block"><sup>2</sup>Southern University of Science and Technology,<br>(南方科技大学)</span>
              <br>
              <span class="author-block"><sup>3</sup>Sun Yat-sen University<br>(中山大学)</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">†Corresponding Author</span>
              <span class="author-block">, *Equal Contribution</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2408.10899.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2408.10899" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://openi.pcl.ac.cn/ARIO" class="external-link button is-normal is-rounded is-dark">
                    <span>Dataset</span>
                  </a>
                  <a href="static/agilex cobot-magic data collection tasks.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span>Full Task List</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/overview.png" alt="MY ALT TEXT" />
        <div class="content has-text-justified">
          <br>
          <div class="columns is-centered has-text-centered">
            <b>Overview of ARIO Dataset</b>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Embodied AI is transforming how AI systems interact with the physical world, yet existing datasets are
              inadequate for developing versatile, general-purpose agents. These limitations include a lack of
              standardized formats, insufficient data diversity, and inadequate data volume. To address these issues, we
              introduce ARIO (All Robots In One), a new data standard that enhances existing datasets by offering a
              unified data format, comprehensive sensory modalities, and a combination of real-world and simulated data.
              ARIO aims to improve the training of embodied AI agents, increasing their robustness and adaptability
              across various tasks and environments. Building upon the proposed new standard, we present a large-scale
              unified ARIO dataset, comprising approximately 3 million episodes collected from 258 series and 321,064
              tasks. The ARIO standard and dataset represent a significant step towards bridging the gaps of existing
              data resources. By providing a cohesive framework for data collection and representation, ARIO paves the
              way for the development of more powerful and versatile embodied AI agents, capable of navigating and
              interacting with the physical world in increasingly complex and diverse ways.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <!-- Paper pipeline. -->
            <h2 class="title is-3">The ARIO Dataset</h2>
            <div class="content has-text-justified">
              <p>
                <b>Comparison between ARIO dataset and other open source embodied datasets</b>
              </p>
            </div>
            <img src="static/images/comparison.png" style="width: 100%; height: auto;" />
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Dataset Collection pipeline -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <!-- Paper pipeline. -->
            <h2 class="title is-3">Data collection pipeline</h2>
            <div class="content has-text-justified">
              <p>
                <b>ARIO data comes from three main sources: conversion from open source datasets, generation from
                  simulation platforms, and acquisition of real-world robot scenarios in our hardware platform.
              </p>
            </div>
            <img src="static/images/pipeline.png" alt="MY ALT TEXT" />
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Model -->

  <!-- Dataset Collection pipeline -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <!-- Paper pipeline. -->
            <h2 class="title is-3">Statistics</h2>
            <div class="content has-text-justified">
              <p>
                Statistics of the ARIO Dataset
              </p>
            </div>
            <img src="static/images/statistic.png" alt="MY ALT TEXT" style="width: 80%; height: auto;" />
            <div class="content has-text-justified">
              <p>
                <b>Statistics on Scenes and Skills in the ARIO Dataset.</b> Thanks to the uniform format design of ARIO
                data, we can easily perform statistical analysis of its data composition. The following figure shows the
                statistical distribution of ARIO's scenes (Figure a) and skills (Figure b) in three levels: series,
                task, and episode. It can be seen that most of the present embodied data is focused on the scenes and
                skills in the indoor living house environment.
              </p>
            </div>
            <img src="static/images/statistic2.jpg" alt="MY ALT TEXT" />
            <div class="content has-text-justified">
              <p>
                In addition to scenes and skills, in the ARIO data, we can also carry out statistical analysis from the
                perspective of the robot itself, and learn some of the current developments in the robot industry. We
                provide statistical data of morphologies, motion objects, physical variables, sensors, sensor positions,
                camera(RGBD) numbers, proportion of control methods, proportion of operation modes, proportion of arm
                joint numbers, corresponding to Figure a-i below. As shown in Figure a below, it can be found that most
                of the current data comes from single-armed robots, while the open source data volume of humanoid robots
                is very small and mainly comes from our real scenario collection and simulation generation.
              </p>
            </div>
            <img src="static/images/statistic1.png" alt="MY ALT TEXT" />
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Model -->

  <section class="section hero is-light">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <!-- Paper pipeline. -->
            <h2 class="title is-3">Examples</h2>
            <div class="content has-text-justified">
              <p>
                <b>One part of the data from real-world scenarios is collected in Cobot Magic (AgileX Robotics)
                  platform. We design over 30 tasks, featuring table-top manipulation in household settings. The tasks
                  cover not only general pick and place skills, but also more complex skills like twist, insert, press,
                  cut, etc. Some example of the tasks can be viewed in figure and video below. Please refer to <a
                    href="static/agilex cobot-magic data collection tasks.pdf">this link</a> for a complete list of
                  tasks. Below are some example tasks in Cobot Magic, with the top row indicating the task category
                  while the text at the botom row providing task instructions.
              </p>
            </div>
            <img src="static/images/example.png" alt="MY ALT TEXT" />
          </div>
        </div>
      </div>

      <div class="container2">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item2">
            <div class="video-container">
              <figure>
                <video controls>
                  <source src="static/images/video1.mp4" type="video/mp4">
                </video>
                <figcaption>Real-world scenario with Cobot Magic Hardware</figcaption>
              </figure>
            </div>
          </div>
          
          <div class="item2">
            <div class="video-container">
              <figure>
                <video controls>
                  <source src="static/images/video2.mp4" type="video/mp4">
                </video>
                <figcaption>Simulation with Dataa SeaWave Software</figcaption>
              </figure>
            </div>
          </div>

          <div class="item2">
            <div class="video-container">
              <figure>
                <video controls>
                  <source src="static/images/video3.mp4" type="video/mp4">
                </video>
                <figcaption>Navigation simulation with Habita-sim platform</figcaption>
              </figure>
            </div>
          </div>

          <div class="item2">
            <div class="video-container">
              <figure>
                <video controls>
                  <source src="static/images/video4.mp4" type="video/mp4">
                </video>
                <figcaption>Pick&Place simulation with MuJoCo engine</figcaption>
              </figure>
            </div>
          </div>

          <div class="item2">
            <div class="video-container">
              <figure>
                <video controls>
                  <source src="static/images/video5.mp4" type="video/mp4">
                </video>
                <figcaption>RH20T example for transformation</figcaption>
              </figure>
            </div>
          </div>

          <div class="item2">
            <div class="video-container">
              <figure>
                <video controls>
                  <source src="static/images/video6.mp4" type="video/mp4">
                </video>
                <figcaption>Open-X Embodiment example for transformation</figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{wang2024robotsonenewstandard,
          title={All Robots in One: A New Standard and Unified Dataset for Versatile, General-Purpose Embodied Agents}, 
          author={Zhiqiang Wang and Hao Zheng and Yunshuang Nie and Wenjun Xu and Qingwei Wang and Hua Ye and Zhe Li and Kaidong Zhang and Xuewen Cheng and Wanxi Dong and Chang Cai and Liang Lin and Feng Zheng and Xiaodan Liang},
          year={2024},
          eprint={2408.10899},
          archivePrefix={arXiv},
          primaryClass={cs.RO},
          url={https://arxiv.org/abs/2408.10899}, 
        }    
      </code></pre>
    </div>
  </section>

  <!-- Ack -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Acknowledgement</h2>
          <div class="content has-text-justified">
            <p>
              This work was supported by Research Institute of Multiple Agents and Embodied Intelligence, Peng Cheng
              Laboratory. We would like to thank Professor Liang Lin, Professor Feng Zheng, and Professor Xiaodan Liang
              for their indispensable support on building ARIO dataset. We also thank Engineer Hua Ye for designing the
              ARIO format, developing simulation and transformation program and managing the project processes, Wenjun
              Xu for managing and supporting data acquisition in real scenarios, Zhiqiang Wang, Qingwei Wang, Hao Zheng
              and Wanxi Dong for transforming Open X-Embodiment, Xuewen Cheng and Tingting Shen for transforming RH20T,
              Yunshuang Nie for collecting navigation data on Habitat simulation, Kaidong Zhang for collecting data on
              Dataa SeaWave simulation, Jinrui Zhang for building this website. A group of people participated in the
              data collection of real robot scenarios, they are Zhe Li, Zhen Luo, Fangjing Wang, Zesheng Yang, Luyang
              Xie, Liang Xu, Yi Yan, Jinrui Zhang, Yixuan Yang, Jingnan Luo, Tongsheng Ding, Ziwei Chen, Guoyu Xiong, Xi
              Jiang, Tiantian Geng, Zhenhong Guo, Xue Jiang, Zhengyu Lin, Ziling Liu, Junfan Lin, Chang Cai, Qingyong
              Jia, Yazhan Zhang, Jichang Li, Bingyi Xia, Jingyi Liu, Shiwei Zhang, Yun Pei, Yao Xiao, Hua Ye, Xuewen
              Cheng.

              We also extend our gratitude to the various open-source, datasets and platforms, including Open
              X-Embodiment, RH20T, ManiWAV, JD ManiData, and the contributors from Open X-Embodiment. Their
              contributions were vital in creating the ARIO dataset. Special thanks to the Habita-sim simulation
              platform, Habitat-lab module library, Habitat-Matterport 3D Dataset (HM3D) indoor dataset, and the Habitat
              Challenge organized by Facebook AI Research. It is through your open-source support that we were able to
              collect navigation simulation data. We are grateful for the Scaling Up and Distilling Down project for the
              simulation framework and the MuJoCo physics engine, aiding in generating simulation manipulation data. We
              appreciate the ARIO Embodied Intelligence Data Open Alliance members, such as Southern University of
              Science and Technology, Sun Yat-sen University, Dataa Robotics, Agilex Robotics, and JD Technology, for
              their technical support and contributions to the ARIO dataset development. The collaborative efforts have
              significantly advanced embodied AI research through the creation of the ARIO dataset.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Ack -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">License</h2>
          <div class="content has-text-justified">
            <p>
              For the real robot data collected by our team or materials generated by the simulation platform developed
              by us, are licensed under the Creative Commons Attribution 4.0 International License (CC-BY) or MIT. For
              materials converted from open source datasets or generated by simulation platform developed by others, we
              just follow their original license while publishing. For details, pay attention to the license of each
              open source project.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">About ARIO Alliance</h2>
          <div class="content has-text-justified">
            <p>
              In March 2024, An alliance organization for building open-source data for embodied intelligence has been
              established, and that is ARIO Alliance. The ARIO Alliance currently has 10 member units and they are
              Pengcheng Laboratory, Southern University of Science and Technology, Sun Yat-sen University, Agilex
              Robotics, University of Hong Kong, Chinese University of Hong Kong, Technical University of Munich, Dataa
              Robotics, D-Robotics, JD Technology. And there are still members coming in. The ARIO Alliance aims to
              promote the academic prosperity and industry development of embodied intelligence by building open source
              embodied datasets and algorithmic models, as well as industry standards.
            </p>
            <img src="static/images/logo.png" style="width: 100%; height: auto;">
          </div>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>